import sys
import numpy as np
from control_loop import ControlLoop

sys.path.append('../../duckietown-sim/')
from gym_duckietown.envs import DuckietownEnv

from diffopt import Deterministic, Gaussian, UnitGaussian, Quadratic
from diffopt import iterative_lqr, cem
import tensorflow as tf
import duckietown_dynamics

'''
ControlLoop Iterative LQR Implementation
Use iLQR to compute actions to reach the next waypoint.
State: [x, y, heading angle] (numpy array)
Action: [velocity, steering angle]

Using Brandon Trabucco's Implementation of iLQR with tensorflow at https://github.com/brandontrabucco/diffopt
Feel free to ignore any confusing tf manipulation :)

LQR:
    Input:
      Quadratatic cost   c = (x-g)T Q (x-g) + uT R u    (x is state, g is goal, u is action)
      Linear dynamics    x_(t+1) = Ax_t + Bu_t
      Initial state      x_0
      Lookahead          T number of timesteps to roll out
    Compute optimal actions that minimize the sum of the cost at each timestep
      Internal math solves for when the gradient of the cost is 0
    Output:
      controls_model is a function that finds the optimal action u_t given time t since x_0 and the current state x_t

Iterative LQR:
    Input:
      Quadratatic cost   c = (x-g)T Q (x-g) + uT R u    (x is state, g is goal, u is action)
      Nonlinear dynamics x_(t+dt) = f(x_t, u_t, dt)
      Initial state      x_0
      Initial policy     function for choosing actions (initialize as random)
      Lookahead          T number of timesteps to roll out
    Initialize states and controls model (initial action policy)
      compute u_0 using policy(x_0)
      generate x_1 using dynamics(x_0, u_0)
      repeat until we have x_0, ..., x_T and u_0, ..., u_T
    Loop num_iter
      Linearly approximate dynamics at time t around x_t, u_t
      Run LQR to find new controls model
      Update x_0, ..., x_T and u_0, ..., u_T with new controls model
    Output:
      controls model
'''
class iLQR(ControlLoop):

    def __init__(self, cost_model, dynamics_model, init_controls_model,
                waypoints, dt, threshold=0.1, debug=False,
                lookahead=10, max_rollout=5, num_iter=20, trust_region=0.1, deterministic=True):
        ControlLoop.__init__(self, waypoints=waypoints, dt=dt, threshold=threshold, debug=debug)

        self.lookahead = lookahead
        self.max_rollout = min(max_rollout, lookahead)
        self.num_iter = num_iter
        self.trust_region = trust_region
        self.deterministic = deterministic
        
        # Quadratic cost returns number given state, action
        self.cost_model = cost_model
        # Model of physics. Returns next state given time, [state, action] (in batches, so there's an extra dim we don't use)
        self.dynamics_model = dynamics_model
        # Policy. Returns action given time, state (not necessarily deterministic)
        self.controls_model = init_controls_model

        # Keep track of t since iLQR with lookahead was computed. If we pass max_rollout, recompute iLQR at the current state.
        self.t_since_ilqr = -1

    def set_waypoint(self, waypoint):
        ControlLoop.set_waypoint(self, waypoint)

        # Pad waypoint [x,y] with a zero to make it a state vector [x,y,theta]
        self.waypoint_as_state = tf.cast(tf.expand_dims(tf.pad(waypoint, tf.constant([[0,1]])), axis=0), tf.float32)

        # Reset t since iLQR to -1, so that iLQR will be recomputed the next call to calc_action.
        self.t_since_ilqr = -1

    def compute_ilqr(self, state):
        zeroed_state = state - self.waypoint_as_state

        # A policy, returns an action given a state.
        self.controls_model = iterative_lqr(
            zeroed_state,
            self.controls_model,
            self.dynamics_model,
            self.cost_model,
            h=self.lookahead,
            n=self.num_iter,
            a=self.trust_region,
            deterministic=self.deterministic
        )

        self.t_since_ilqr = 0

    def calc_action(self, state):
        if self.t_since_ilqr >= self.max_rollout or self.t_since_ilqr == -1:
            self.compute_ilqr(state)

        zeroed_state = state - self.waypoint_as_state
        action, _ = self.controls_model.expected_value(time=self.t_since_ilqr, inputs=zeroed_state)
        self.t_since_ilqr += 1

        action = [float(ai) for ai in action[0]]

        if self.debug:
            print("State: {}, Waypoint: {}, Action: {}".format(state, self.cur_waypoint, action))

        return action

if __name__ == "__main__":
    # Example trajectory, as would be generated by path planning
    trajectory = [np.array([2., 1.]), np.array([2., 3.]), np.array([1., 3.]), np.array([1.,1.])]

    env = DuckietownEnv(map_name='udem1', user_tile_start=(1, 1), domain_rand=False,
                        init_x=0.75, init_z=0.75, init_angle=0.)
    env.reset()
    env.render(top_down=True)

    # ======= DYNAMICS =========

    # Point-mass physics model
    # inputs: (n x dim_state), (n x dim_actions), float
    # output: next_states (n x dim_state)
    def simplified_dynamics(states, actions, dt):
        actions = tf.clip_by_value(actions, -1., 1.)
        next_states = [states + tf.stack([
            actions[:, 0] * tf.cos(states[:, 2]) * dt,
            actions[:, 0] * -tf.sin(states[:, 2]) * dt,
            actions[:, 1] * dt], 1)]
        return next_states
    simplified_dynamics_model = Deterministic(lambda time, inputs: simplified_dynamics(inputs[0], inputs[1], env.delta_time))

    # Full dynamics model, using simulation implementation
    complex_dynamics_model = duckietown_dynamics.Dynamics(env).dynamics_model


    # ======== COST ==========

    ######################################
    #       Specify a Cost Function       #
    ######################################
    '''
    Cost = sum_{time=0 to T} (x-g)T Q (x-g) + uT R u
     x = state  [x, y, theta]
     g = goal   [x, y, 0]
     u = action [vel, steering]
    Notes:
        These each have different units and costs.
        Specify Q and R matrices to properly weight each term.
        They're usually diagonal, considering each dimension independently.
        We are only given waypoints as coordinates. This means we should either disregard theta in the cost,
        or compute target orientations for the car at each waypoint.
    '''
    Q = tf.constant([
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]], dtype=tf.float32)
    R = tf.constant([
        [0, 0],
        [0, 0]], dtype=tf.float32)

    # Compute (c = xT Q x + uT R u) by concatenating x with u, and using one big matrix
    cost_model = Quadratic(0, [0, 0], [0, 0], [[Q, 0], [0, R]])


    # ======= INITIAL POLICY =========
    
    # Initialize policy at always cruising forward
    controls_model = Deterministic(lambda time, inputs: [tf.constant([[0.2, 0.]])])

    # ======== CONTROL LOOP ==========
    # Choose which dynamics model to use. The simplified one is faster but less accurate.
    dynamics_model = simplified_dynamics_model
    # dynamics_model = complex_dynamics_model

    # Feel free to experiment with the hyperparameters
    control_loop = iLQR(cost_model=cost_model, dynamics_model=dynamics_model, init_controls_model=controls_model,
                        waypoints=trajectory, dt=env.delta_time, threshold=0.2, debug=True,
                        lookahead=10, max_rollout=9, num_iter=30, trust_region=0.1, deterministic=True)
    while not control_loop.is_finished:
        state = [env.cur_pos[0], env.cur_pos[2], env.cur_angle]

        action = control_loop.get_action(state)

        env.step(action)
        env.render(top_down=True)

    print("FINISHED")
